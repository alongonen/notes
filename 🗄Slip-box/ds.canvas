{
	"nodes":[
		{"type":"group","id":"760eb940ceb3b4d5","x":-5651,"y":1120,"width":2051,"height":1360,"label":"Linearly Fitting Quadratic Functions using Random FeaturesÂ "},
		{"type":"group","id":"8c71ee0d0e9f8794","x":-3200,"y":-380,"width":1820,"height":1200,"label":"More Data Hurts?"},
		{"type":"group","id":"eb5508190ac52270","x":-5651,"y":-120,"width":1680,"height":680,"label":"Bias-Variance: Traditional vs. Modern ML"},
		{"type":"text","text":"## Linear Regression\n\n$$\nX = \\begin{pmatrix} | ~~~~~~ ~~~~~~ | \\\\ x_1 ~\\cdots ~ x_n \\\\ | ~~~~~~~~~~~~ | \\end{pmatrix} \\in \\mathbb{R}^{d \\times n}\n$$\n$$\n\\begin{align*}\n&\\min f(w) = \\frac{1}{2}\\|w^\\top X - Y\\|^2 \\Leftrightarrow \\\\\n&\\min \\frac{1}{2}w^\\top Aw-b^\\top w\n\\end{align*}\n$$\nwhere $b = Xy,~A=XX^\\top$.\n\n### Opt\n$$\nw^\\star = A^{\\dagger} b = X^\\dagger y\n$$\n\n\n### Recovery conditions\n$X = \\sum_{i=1}^r \\sigma_i u_i v_i^\\top\\,\\,\\,,\\,A=\\sum_{i=1}^r \\underbrace{\\lambda_i}_{\\sigma_i^2} u_i u_i^\\top, \\, y=\\sum_{i=1}^k \\beta_i v_i$     ($k$ might be smaller, equal or larger than $r$)\n$b = Xy = \\sum_{i=1}^{\\min \\{k,r\\}} \\sum \\beta_i \\sigma_i u_i$.\n\n$$\nA ^\\dagger b=\\sum_{i=1}^{\\min \\{k,r\\}} \\sigma_i^{-1} \\beta_i u_i = X^\\dagger y\n$$\n$$\nX^\\top w^\\star = \\sum_{i=1}^{\\min \\{k,r\\}} \\beta_i v_i \n$$\nThat is, $X^\\top w^\\star$ is the projection of $y$ onto the span of $X^\\top$. It is equal to $y$ if and only if $r \\ge k$.","id":"c55af9fb4f22b11e","x":-3160,"y":-340,"width":840,"height":360},
		{"type":"file","file":"ðŸ—„Slip-box/Min Singular Value of Gaussian Matrix.md","id":"87bcce5ea73957b2","x":-2240,"y":-340,"width":800,"height":1020},
		{"type":"text","text":"![[one-dim ds#^frame=vYmXvKUhjyRJ2E5bzDa29|data for one-dim random features]]","id":"7654669bc000d34f","x":-4853,"y":1169,"width":1219,"height":1262},
		{"type":"text","text":"\n\n- how noise affect this example?\n- drawings for K> 4","id":"8c5650dbd5701b74","x":-5531,"y":1328,"width":423,"height":147},
		{"type":"text","text":"![[ðŸŽ¨Excalidraw/double descent/bias-variance.md#^frame=kN2aDJNOKtWIG_hcZauKY|model bias variance|700]]\n","id":"2e20e8ceaaebaa2f","x":-4691,"y":-80,"width":640,"height":548},
		{"type":"text","text":"![[ðŸŽ¨Excalidraw/double descent/bias-variance.md#^frame=_kmM0rxJWw2LI-vGCRbsq|traditional bias-variance|500]]\n","id":"44dd4e026134fb24","x":-5571,"y":-80,"width":800,"height":548},
		{"id":"8cb9c689db333fb7","x":-2740,"y":1860,"width":1300,"height":380,"type":"text","text":"Deep Double Descent"}
	],
	"edges":[]
}
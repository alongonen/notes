

## Summarization

### Model Sizes

**T5**
![](https://i.imgur.com/C1Zx9eT.png)

- fp16, distillation


- tiny bert
- [gist for distil-bert](https://gist.github.com/remi-or/4814577c59f4f38fcc89729ce4ba21e6)
- 



### Known Results
- [BART](https://github.com/facebookresearch/fairseq/blob/6f7b7d202199dc1534a77a247da1ad4604c21baa/examples/bart/README.md)
![](https://i.imgur.com/kVs4qIB.png)
- [Pegasus](https://huggingface.co/google/pegasus-cnn_dailymail)

![](https://i.imgur.com/UVGUhh6.png)



### Finetune receipts

- [Finetuning BART](https://github.com/facebookresearch/fairseq/issues/1364)
- [colab notebook with t5-base](https://colab.research.google.com/drive/1-wp_pRVxl6c0Y0esn8ShIdeil3Bh854d?usp=sharing#scrollTo=-U5-lpUtxwla)
- [towards data science](https://knswamy.medium.com/nlp-deep-learning-training-on-downstream-tasks-using-pytorch-lightning-summarization-on-xsum-3b4ffd5db91d)
- 

